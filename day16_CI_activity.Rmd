---
title: "Day 16: CIs for Means"
author: "Math 275, St Clair"
output: html_document
---

```{r, include=FALSE}

knitr::opts_chunk$set(collapse=TRUE, prompt=TRUE, comment=NULL, eval=FALSE, warning=FALSE, message=FALSE)
```

### Example 1: Service times revisted
The textbook data set Service contains service times for 174 customers of a snack bar. In hw 6 and 7 you modeled these times as gamma random variables and estimated the gamma parameters. Here we will assume these 174 times are a sample from all possible customers and $\mu$ represents the mean service time for this population. 

```{r}
library(resampledata)
data("Service")
head(Service)
```

#### (a) Plot the `Times` data and describe its shape, then compute the sample mean and SD. 


#### (b) Compute the SE of the sample mean.


#### (c) Using the classic t CI, compute the margin of error for 95% confidence using the `qt` function (not `t.test`). 


#### (d) Repeat (c) for 99% confidence. Which margin of error is bigger? Why? How would a 90% margin of error compare?


#### (e) Use `t.test` to compute a 99% confidence interval for $\mu$. Verify that the margin of error matches your answer from (d) and interpret the interval in context. 


#### (f) Construct a 99% bootstrap t confidence interval for $\mu$. Compare it to your classic t CI by comparing the margin of error to the left and right of the sample mean time.

Here is the starter code for this example:
```{r}
N <- 20000
T_boot <-  rep(NA,N)
n<- length(Service$Times) 
set.seed(7037479)
for (i in 1:N)
{
  x_boot <- sample(Service$Times, n, replace=TRUE)
  T_boot[i] <- (mean(x_boot) - mean(Service$Times))/(sd(x_boot)/sqrt(n))
}
```



#### (g) Use a t QQ plot to compare your bootstrap T stats to the t distribution with 173 degrees of freedom. Comment on whether you can trust the classic t CI or whether you should use the bootstrap t for inference purposes. 

Here is the basic code needed. 

```{r}
library(car)
qqPlot(T_boot, dist = "t", df=n-1, envelope=FALSE)
```



### Example 2: Kickstarter goal amounts
The data `kickstarterSample.csv` contains a random sample of 300 failed and 200 successful Kickstarter projects from the US in 2016 that had goal amounts below 1 million dollars. 
```{r data}
ks <- read.csv("http://math.carleton.edu/kstclair/data/kickstarterSample.csv")
table(ks$state)
```
Consider comparing the mean goal amount for successful and failed projects. 

#### (a) Draw a boxplot to compare distribution of `goal` amounts by `state`. Compute the sample mean and SD for each group.


#### (b) Compute a 95% classic t CI for $\mu_{fail} - \mu_{success}$, the difference in mean goal amounts for the failed and successful projects in the population. Use `t.test(y~x,data=)` where `y` is the response and `x` is the grouping variable. What is the margin of error? Do you think assumptions are met to trust this CI?


#### (c) Compute and interpret a 95% bootstrap t CI for $\mu_{fail} - \mu_{success}$. Plot the distribution of the bootstrap T statistic (including a QQ-t plot like used in (1g)) and comment on which interval, the classic or bootstrap t, should be used for inference. 

Here is starter code to extract the goal amounts for each type of project. You can copy/modify the code from (1f) to create your bootstrap T simulation.

```{r}
goal_fail <- ks %>% 
  filter(state == "failed") %>% 
  pull(goal)
goal_success<- ks %>% 
  filter(state == "successful") %>% 
  pull(goal)
```

#### (d) When we compare two parameters/stats by a difference, it is arbitrary what we substract from what. In `t.test`, R will compute the difference as `failed - successful` because of the alphabetical ordering of the levels. But we will get the same CI, just with a different sign, if we reverse the direction of the difference. To see this, plot the distribution of the bootstrap T statistic for the population difference $\mu_{success} - \mu_{fail}$. How does it differ from the distribution in part (c)? Note: you don't need to rerun the bootstrap simulation. Just convince yourself via some algebra that the bootstrap T stat for $\mu_{success} - \mu_{fail}$ is just the *negative* of the bootstrap T from part (c).


#### (e) Use the original population of 2016 kickstarter projects to conduct a simulation study of the classic t and bootstrap t CIs for $\mu_{fail} - \mu_{success}$ based on samples of 300 failed projects and 200 successful projects. Compare coverage rates for both and use a coverage rate of 95%. 

Here is the population of cases  that had goal amounts below $1 million dollars:
```{r}
population <-  read.csv("http://math.carleton.edu/kstclair/data/kickstarterUS16.csv") %>% filter(goal < 1000000)
# cases with less than $1 million goal
pop_fail <-population %>%
  filter(state == "failed") %>%
  pull(goal)
pop_success <- population %>%
  filter(state ==  "successful") %>%
  pull(goal)
mean(pop_fail)
mean(pop_success)
# PARAMETER OF INTEREST: pop diff in means
(pop_diff <- mean(pop_fail) - mean(pop_success) )  
```

Here is a template for the simulation:

Simulation values:
```{r}
N_sim <- 2000  # keep this smallish or this will take a while!
n_f <- 300  # sample size for failed
n_s <- 200   # sample size for successful
# where to store the lower/upper bounds of your CI:
low_boot <- low_t <- rep(NA,N_sim)  # lower CI bound
up_boot <- up_t <- rep(NA,N_sim)  # upper CI bound
```

Bootstrap values (for each simulated sample)
```{r}
N <- 1000  # also keep smaller than usual for time!
T_boot <- rep(NA,N) 
```

Run simulation (fill in the needed code!)
```{r, cache=TRUE}
for (i in 1:N_sim)
{ 
# STEP 1: pick a random sample from the population:
  # sample without replacement from pops (SRS)
  # use sample sizes n_f = 300 and n_s = 200
  x_f <- sample(pop_fail, n_f, replace=FALSE)
  x_s <- sample(pop_success, n_s, replace = FALSE)
  
# STEP 2: Compute the "classic" t-CI and store upper/lower bounds in low_t and up_t
  # get t CI using t.test(your failed data, your success data)$conf

# STEP 3: Compute the bootstrap-t CI and store upper/lower bounds in low_boot and up_boot
  
}

```




